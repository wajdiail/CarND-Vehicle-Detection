{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies required for this project\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import time \n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing car and noncars image location to variable for reading later\n",
    "\n",
    "cars = glob.glob('vehicles/**/*.png')\n",
    "noncars = glob.glob('non-vehicles/**/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting HOG features\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image= hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=False, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=False, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating historgram of color features for a given iamge:\n",
    "\n",
    "def color_hist(img, nbins=32):    #bins_range=(0, 256)\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a function to extract features from a list of image locations\n",
    "# This function could also be used to call bin_spatial() and color_hist() (as in the lessons) to extract\n",
    "# flattened spatial color features and color histogram features and combine them all (making use of StandardScaler)\n",
    "# to be used together for classification\n",
    "def extract_features(imgs, cspace='RGB', orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        # Read in each one by one\n",
    "        \n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if cspace != 'RGB':\n",
    "            if cspace == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif cspace == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif cspace == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif cspace == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif cspace == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))\n",
    "            hog_features = np.ravel(hog_features)        \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            \n",
    "        hist_features=color_hist(feature_image,32)\n",
    "        final_features=np.concatenate((hist_features, hog_features))\n",
    "        # Append the new feature vector to the features list\n",
    "        features.append(final_features)\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "print('...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 Seconds to extract features...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Feature extraction parameters\n",
    "colorspace = 'HSV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 8\n",
    "hog_channel = 'ALL' # Can be 0, 1, 2, or \"ALL\"\n",
    "\n",
    "t = time.time()\n",
    "car_features= extract_features(cars, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "notcar_features= extract_features(noncars, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to extract features...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)  \n",
    "\n",
    "# Fit a per-column scaler - this will be necessary if combining different types of features (HOG + color_hist/bin_spatial)\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t = time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cars(img, svc, X_scaler, pix_per_cell, cell_per_block,  ystart, ystop, xstart,xstop,scale, orient, spatial_size, hist_bins):\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    #Extract the search area of the image\n",
    "    img_tosearch = img[ystart:ystop,xstart:xstop,:]\n",
    "    \n",
    "    #Conver the to LUV color space\n",
    "    ctrans_tosearch = cv2.cvtColor(img_tosearch, cv2.COLOR_RGB2LUV)\n",
    " \n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    #Separating the 3 image channels for further processing\n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "    \n",
    "    #Preparing for generating hog features\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    \n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    \n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    " \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "\n",
    "    boxes =[]\n",
    "    \n",
    "    for xb in range(nxsteps):        \n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            \n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "            \n",
    "            # Extract the image patch\n",
    "            subimg = ctrans_tosearch[ytop:ytop+window, xleft:xleft+window]\n",
    "            \n",
    "            #Get histogram of color features for the path\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "         \n",
    "            #Combine hog features and hist features\n",
    "            img_features= np.concatenate((hist_features, hog_features))\n",
    "            \n",
    "            #Condition and normalize the combined features\n",
    "            img_features = final_features.reshape(1,-1)\n",
    "            final_features = X_scaler.transform(img_features) \n",
    "            \n",
    "            #Pass the final features to the classifier to get the prediction\n",
    "            prediction = svc.predict(final_features)\n",
    "            \n",
    "            #Generate detected points\n",
    "            if prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                pts = ((xbox_left+xstart, ytop_draw+ystart),(xbox_left+win_draw+xstart,ytop_draw+win_draw+ystart))\n",
    "                boxes.append(pts)\n",
    "                \n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBoxes ():\n",
    " #Class for merging the overlapped boxes into single bounding box and implicitly eliminating false positives\n",
    "\n",
    "    def __init__ (self, box):\n",
    "        self.merged_box = [list(p) for p in box]\n",
    "        self.detected_count = 1\n",
    "        self.boxes = [box]\n",
    "        \n",
    "    def get_box (self):\n",
    "    #Generates a box that has average center of all boxes and have size of 2 standard deviation by x and y \n",
    "    #coordinates of its points\n",
    "\n",
    "        if len(self.boxes) > 1:\n",
    "            center = np.average (np.average (self.boxes, axis=1), axis=0).astype(np.int32).tolist()\n",
    "\n",
    "            # getting all x and y coordinates of all corners of joined boxes separately\n",
    "            xs = np.array(self.boxes) [:,:,0]\n",
    "            ys = np.array(self.boxes) [:,:,1]\n",
    "\n",
    "            half_width = int(np.std (xs))\n",
    "            half_height = int(np.std (ys))\n",
    "            return ((center[0] - half_width,center[1] - half_height), (center[0] + half_width,center[1] + half_height))\n",
    "        else:\n",
    "            return self.boxes [0]\n",
    "        \n",
    "\n",
    "    def check_close (self, box):\n",
    "    #Check if specified box is close enough for merging i.e by 30% of area of this box or the average box  \n",
    "    \n",
    "        x11 = self.merged_box [0][0]\n",
    "        y11 = self.merged_box [0][1]\n",
    "        x12 = self.merged_box [1][0]\n",
    "        y12 = self.merged_box [1][1]\n",
    "        x21 = box [0][0]\n",
    "        y21 = box [0][1]\n",
    "        x22 = box [1][0]\n",
    "        y22 = box [1][1]\n",
    "            \n",
    "        x_overlap = max(0, min(x12,x22) - max(x11,x21))\n",
    "        y_overlap = max(0, min(y12,y22) - max(y11,y21))\n",
    "\n",
    "        area1 = (x12 - x11) * (y12 - y11)\n",
    "        area2 = (x22 - x21) * (y22 - y21)\n",
    "        intersection = x_overlap * y_overlap;\n",
    "        \n",
    "        if (intersection >= 0.3 * area1 or intersection >= 0.3 * area2):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def merge (self, boxes):\n",
    "    #Merge in all boxes from list of given boxes and removes merged boxes from input list of boxes\n",
    "        merged = False\n",
    "        \n",
    "        for b in boxes:\n",
    "            if self.check_close (b):\n",
    "                boxes.remove (b)\n",
    "                self.boxes.append (b)\n",
    "                self.detected_count += 1\n",
    "                \n",
    "                self.merged_box[0][0] = min(self.merged_box[0][0], b[0][0])\n",
    "                self.merged_box[0][1] = min(self.merged_box[0][1], b[0][1])\n",
    "                self.merged_box[1][0] = max(self.merged_box[1][0], b[1][0])\n",
    "                self.merged_box[1][1] = max(self.merged_box[1][1], b[1][1])\n",
    "                \n",
    "                merged = True\n",
    "\n",
    "        return merged\n",
    "\n",
    "class PredictionHistory():\n",
    "#class for storing and retriving prediction history from last 10 frames.\n",
    "    def __init__ (self):\n",
    "        self.history = 10\n",
    "        self.box_hist = []\n",
    "\n",
    "    def to_prev_boxes (self, boxes):\n",
    "    #Function for storing the history  \n",
    "        if (len(self.box_hist) > self.history):\n",
    "            temp = self.box_hist.pop (0)\n",
    "        \n",
    "        self.box_hist.append (boxes)\n",
    "        \n",
    "    def from_prev_boxes (self):\n",
    "    #Function for retrieving the history\n",
    "        box = []\n",
    "        for boxes in self.box_hist:\n",
    "            box.extend (boxes)\n",
    "        return box\n",
    "    \n",
    "#Instantiating a class object\n",
    "tracker = PredictionHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_boxes(boxes, threshold):\n",
    "#Compute merged boxes from detected muliple boxes using the algorithm defined in the class MergeBoxes\n",
    "    merged_boxes = []\n",
    "    while len(boxes) > 0:\n",
    "        box = boxes.pop (0)\n",
    "        box2 = MergeBoxes(box)\n",
    "        while box2.merge (boxes):\n",
    "            pass\n",
    "        merged_boxes.append (box2)\n",
    "    \n",
    "    boxes = []\n",
    "    for b in merged_boxes:\n",
    "        if b.detected_count >= threshold:\n",
    "            boxes.append (b.get_box ())\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_pipeline(img):\n",
    "    \n",
    "    image =  np.copy(img)\n",
    "    \n",
    "    \n",
    "    # Sliding window search on sclae =1 to get the detection boxes\n",
    "    boxes1 = find_cars(image, svc, X_scaler, pix_per_cell, cell_per_block,\\\n",
    "                       ystart=350, ystop=500, xstart=750,xstop=1280,scale=1, orient=9,\\\n",
    "                       spatial_size=(32,32), hist_bins=32)\n",
    "    \n",
    "    # Sliding window search on sclae =1 to get the detection boxes\n",
    "    boxes2 = find_cars(image, svc, X_scaler, pix_per_cell, cell_per_block,\\\n",
    "                       ystart=350, ystop=660, xstart=750, xstop=1280, scale=1.5, orient=9,\\\n",
    "                       spatial_size=(32,32), hist_bins=32)\n",
    "\n",
    "    boxes = []\n",
    "    \n",
    "    #Combining boxes detected from both scales\n",
    "    boxes = boxes1 + boxes2\n",
    "    \n",
    "    #Storing the detected boxes for use in the next frames\n",
    "    tracker.to_prev_boxes(boxes)\n",
    "    \n",
    "    #Getting the boxes from the the history of last 10 frames\n",
    "    boxes = tracker.from_prev_boxes()\n",
    "    \n",
    "    #Combining multiple overlapped boxes into single bounding box also eliminating any fasle positives\n",
    "    merged_boxes = get_merged_boxes(boxes, 20)\n",
    "    \n",
    "    #Drawing bounding boxes on the detected cars\n",
    "    for boxes in merged_boxes:\n",
    "        cv2.rectangle(img,boxes[0],boxes[1], (0,0,255), 5)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to test on a single image\n",
    "#img=cv2.imread(\"./test_images/test3.jpg\")\n",
    "#draw_img=video_pipeline(img)\n",
    "#plt.imshow(draw_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "project_output ='project_output.mp4'\n",
    "#clip1 = VideoFileClip(\"test_video.mp4\")\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "project_clip = clip1.fl_image(video_pipeline) #NOTE: this function expects color images!!\n",
    "%time project_clip.write_videofile(project_output, audio=False)\n",
    "\n",
    "#View the processed video of yellow_output\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
